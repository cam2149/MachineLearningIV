{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 91296,
          "databundleVersionId": 10829413,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30840,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Training",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f628c36e29d3410f957338bc337f399b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_78b8a7885a05404db5c7915a8979339b",
              "IPY_MODEL_a23bf6d16be544b5a0d18b552f368f6f",
              "IPY_MODEL_6fd1af769f894869b9f03d4454b84fdb"
            ],
            "layout": "IPY_MODEL_636677ddefe44f73a7c2a0f82de6ce77"
          }
        },
        "78b8a7885a05404db5c7915a8979339b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e330acc8690e45a88243a3d9cfcb82c1",
            "placeholder": "​",
            "style": "IPY_MODEL_8a1e4aaa42184afb874bc9753b0e9167",
            "value": ""
          }
        },
        "a23bf6d16be544b5a0d18b552f368f6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1be12a41519040daaa20962fc958ee0a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52d776a5f2524167be013412013d1df8",
            "value": 0
          }
        },
        "6fd1af769f894869b9f03d4454b84fdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4c9097ef970436eadc80153f7c33253",
            "placeholder": "​",
            "style": "IPY_MODEL_ec2f3718f69b4ef0bfbc191e295ca66d",
            "value": " 0/? [00:00&lt;?, ?it/s]"
          }
        },
        "636677ddefe44f73a7c2a0f82de6ce77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e330acc8690e45a88243a3d9cfcb82c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a1e4aaa42184afb874bc9753b0e9167": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1be12a41519040daaa20962fc958ee0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "52d776a5f2524167be013412013d1df8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a4c9097ef970436eadc80153f7c33253": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec2f3718f69b4ef0bfbc191e295ca66d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cam2149/MachineLearningIV/blob/main/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "#kagglehub.login()\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "cN-bAvh18Zo3"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Eliminar todo el contenido de la carpeta /content/kaggle\n",
        "\n",
        "!rm -rf /content/kaggle\n"
      ],
      "metadata": {
        "id": "JhfmGMUf8sz4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c aa-iv-2025-i-object-localization\n",
        "print('Data source import complete.')"
      ],
      "metadata": {
        "id": "O6PMswB68zMw",
        "outputId": "6a024d67-b45d-48dd-8902-84167125d27f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading aa-iv-2025-i-object-localization.zip to /content\n",
            " 24% 7.00M/29.1M [00:00<00:00, 71.1MB/s]\n",
            "100% 29.1M/29.1M [00:00<00:00, 172MB/s] \n",
            "Data source import complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Descomprimir el archivo unzip aa-iv-2025-i-object-localization.zip dentro de una carperta llamada kaggle\n",
        "\n",
        "!mkdir kaggle\n",
        "!mkdir kaggle/working\n",
        "!unzip aa-iv-2025-i-object-localization.zip -d kaggle\n"
      ],
      "metadata": {
        "id": "uZ8H_Lme-TOM",
        "outputId": "bafcb1ff-dc47-4b03-b2cd-80f610cf29a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  aa-iv-2025-i-object-localization.zip\n",
            "  inflating: kaggle/images/images/image_00001.jpeg  \n",
            "  inflating: kaggle/images/images/image_00002.jpeg  \n",
            "  inflating: kaggle/images/images/image_00003.jpeg  \n",
            "  inflating: kaggle/images/images/image_00004.jpeg  \n",
            "  inflating: kaggle/images/images/image_00005.jpeg  \n",
            "  inflating: kaggle/images/images/image_00007.jpeg  \n",
            "  inflating: kaggle/images/images/image_00011.jpeg  \n",
            "  inflating: kaggle/images/images/image_00012.jpeg  \n",
            "  inflating: kaggle/images/images/image_00013.jpeg  \n",
            "  inflating: kaggle/images/images/image_00014.jpeg  \n",
            "  inflating: kaggle/images/images/image_00015.jpeg  \n",
            "  inflating: kaggle/images/images/image_00016.jpeg  \n",
            "  inflating: kaggle/images/images/image_00017.jpeg  \n",
            "  inflating: kaggle/images/images/image_00018.jpeg  \n",
            "  inflating: kaggle/images/images/image_00019.jpeg  \n",
            "  inflating: kaggle/images/images/image_00020.jpeg  \n",
            "  inflating: kaggle/images/images/image_00021.jpeg  \n",
            "  inflating: kaggle/images/images/image_00022.jpeg  \n",
            "  inflating: kaggle/images/images/image_00023.jpeg  \n",
            "  inflating: kaggle/images/images/image_00024.jpeg  \n",
            "  inflating: kaggle/images/images/image_00025.jpeg  \n",
            "  inflating: kaggle/images/images/image_00026.jpeg  \n",
            "  inflating: kaggle/images/images/image_00027.jpeg  \n",
            "  inflating: kaggle/images/images/image_00028.jpeg  \n",
            "  inflating: kaggle/images/images/image_00029.jpeg  \n",
            "  inflating: kaggle/images/images/image_00030.jpeg  \n",
            "  inflating: kaggle/images/images/image_00031.jpeg  \n",
            "  inflating: kaggle/images/images/image_00032.jpeg  \n",
            "  inflating: kaggle/images/images/image_00033.jpeg  \n",
            "  inflating: kaggle/images/images/image_00034.jpeg  \n",
            "  inflating: kaggle/images/images/image_00035.jpeg  \n",
            "  inflating: kaggle/images/images/image_00036.jpeg  \n",
            "  inflating: kaggle/images/images/image_00037.jpeg  \n",
            "  inflating: kaggle/images/images/image_00038.jpeg  \n",
            "  inflating: kaggle/images/images/image_00039.jpeg  \n",
            "  inflating: kaggle/images/images/image_00040.jpeg  \n",
            "  inflating: kaggle/images/images/image_00041.jpeg  \n",
            "  inflating: kaggle/images/images/image_00042.jpeg  \n",
            "  inflating: kaggle/images/images/image_00043.jpeg  \n",
            "  inflating: kaggle/images/images/image_00044.jpeg  \n",
            "  inflating: kaggle/images/images/image_00045.jpeg  \n",
            "  inflating: kaggle/images/images/image_00046.jpeg  \n",
            "  inflating: kaggle/images/images/image_00047.jpeg  \n",
            "  inflating: kaggle/images/images/image_00048.jpeg  \n",
            "  inflating: kaggle/images/images/image_00049.jpeg  \n",
            "  inflating: kaggle/images/images/image_00050.jpeg  \n",
            "  inflating: kaggle/images/images/image_00051.jpeg  \n",
            "  inflating: kaggle/images/images/image_00052.jpeg  \n",
            "  inflating: kaggle/images/images/image_00053.jpeg  \n",
            "  inflating: kaggle/images/images/image_00054.jpeg  \n",
            "  inflating: kaggle/images/images/image_00055.jpeg  \n",
            "  inflating: kaggle/images/images/image_00056.jpeg  \n",
            "  inflating: kaggle/images/images/image_00057.jpeg  \n",
            "  inflating: kaggle/images/images/image_00058.jpeg  \n",
            "  inflating: kaggle/images/images/image_00060.jpeg  \n",
            "  inflating: kaggle/images/images/image_00061.jpeg  \n",
            "  inflating: kaggle/images/images/image_00062.jpeg  \n",
            "  inflating: kaggle/images/images/image_00063.jpeg  \n",
            "  inflating: kaggle/images/images/image_00064.jpeg  \n",
            "  inflating: kaggle/images/images/image_00065.jpeg  \n",
            "  inflating: kaggle/images/images/image_00066.jpeg  \n",
            "  inflating: kaggle/images/images/image_00067.jpeg  \n",
            "  inflating: kaggle/images/images/image_00068.jpeg  \n",
            "  inflating: kaggle/images/images/image_00070.jpeg  \n",
            "  inflating: kaggle/images/images/image_00071.jpeg  \n",
            "  inflating: kaggle/images/images/image_00072.jpeg  \n",
            "  inflating: kaggle/images/images/image_00073.jpeg  \n",
            "  inflating: kaggle/images/images/image_00074.jpeg  \n",
            "  inflating: kaggle/images/images/image_00075.jpeg  \n",
            "  inflating: kaggle/images/images/image_00076.jpeg  \n",
            "  inflating: kaggle/images/images/image_00077.jpeg  \n",
            "  inflating: kaggle/images/images/image_00078.jpeg  \n",
            "  inflating: kaggle/images/images/image_00079.jpeg  \n",
            "  inflating: kaggle/images/images/image_00080.jpeg  \n",
            "  inflating: kaggle/images/images/image_00081.jpeg  \n",
            "  inflating: kaggle/images/images/image_00082.jpeg  \n",
            "  inflating: kaggle/images/images/image_00083.jpeg  \n",
            "  inflating: kaggle/images/images/image_00084.jpeg  \n",
            "  inflating: kaggle/images/images/image_00085.jpeg  \n",
            "  inflating: kaggle/images/images/image_00086.jpeg  \n",
            "  inflating: kaggle/images/images/image_00087.jpeg  \n",
            "  inflating: kaggle/images/images/image_00088.jpeg  \n",
            "  inflating: kaggle/images/images/image_00089.jpeg  \n",
            "  inflating: kaggle/images/images/image_00090.jpeg  \n",
            "  inflating: kaggle/images/images/image_00091.jpeg  \n",
            "  inflating: kaggle/images/images/image_00092.jpeg  \n",
            "  inflating: kaggle/images/images/image_00095.jpeg  \n",
            "  inflating: kaggle/images/images/image_00096.jpeg  \n",
            "  inflating: kaggle/images/images/image_00097.jpeg  \n",
            "  inflating: kaggle/images/images/image_00098.jpeg  \n",
            "  inflating: kaggle/images/images/image_00099.jpeg  \n",
            "  inflating: kaggle/images/images/image_00100.jpeg  \n",
            "  inflating: kaggle/images/images/image_00101.jpeg  \n",
            "  inflating: kaggle/images/images/image_00102.jpeg  \n",
            "  inflating: kaggle/images/images/image_00103.jpeg  \n",
            "  inflating: kaggle/images/images/image_00104.jpeg  \n",
            "  inflating: kaggle/images/images/image_00105.jpeg  \n",
            "  inflating: kaggle/images/images/image_00106.jpeg  \n",
            "  inflating: kaggle/images/images/image_00107.jpeg  \n",
            "  inflating: kaggle/images/images/image_00108.jpeg  \n",
            "  inflating: kaggle/images/images/image_00109.jpeg  \n",
            "  inflating: kaggle/images/images/image_00110.jpeg  \n",
            "  inflating: kaggle/images/images/image_00111.jpeg  \n",
            "  inflating: kaggle/images/images/image_00112.jpeg  \n",
            "  inflating: kaggle/images/images/image_00113.jpeg  \n",
            "  inflating: kaggle/images/images/image_00114.jpeg  \n",
            "  inflating: kaggle/images/images/image_00115.jpeg  \n",
            "  inflating: kaggle/images/images/image_00116.jpeg  \n",
            "  inflating: kaggle/images/images/image_00117.jpeg  \n",
            "  inflating: kaggle/images/images/image_00119.jpeg  \n",
            "  inflating: kaggle/images/images/image_00120.jpeg  \n",
            "  inflating: kaggle/images/images/image_00121.jpeg  \n",
            "  inflating: kaggle/images/images/image_00122.jpeg  \n",
            "  inflating: kaggle/images/images/image_00127.jpeg  \n",
            "  inflating: kaggle/images/images/image_00128.jpeg  \n",
            "  inflating: kaggle/images/images/image_00129.jpeg  \n",
            "  inflating: kaggle/images/images/image_00137.jpeg  \n",
            "  inflating: kaggle/images/images/image_00138.jpeg  \n",
            "  inflating: kaggle/images/images/image_00139.jpeg  \n",
            "  inflating: kaggle/images/images/image_00140.jpeg  \n",
            "  inflating: kaggle/images/images/image_00141.jpeg  \n",
            "  inflating: kaggle/images/images/image_00147.jpeg  \n",
            "  inflating: kaggle/images/images/image_00148.jpeg  \n",
            "  inflating: kaggle/images/images/image_00149.jpeg  \n",
            "  inflating: kaggle/images/images/image_00157.jpeg  \n",
            "  inflating: kaggle/images/images/image_00158.jpeg  \n",
            "  inflating: kaggle/images/images/image_00159.jpeg  \n",
            "  inflating: kaggle/images/images/image_00160.jpeg  \n",
            "  inflating: kaggle/images/images/image_00161.jpeg  \n",
            "  inflating: kaggle/images/images/image_00174.jpeg  \n",
            "  inflating: kaggle/images/images/image_00177.jpeg  \n",
            "  inflating: kaggle/images/images/image_00178.jpeg  \n",
            "  inflating: kaggle/images/images/image_00180.jpeg  \n",
            "  inflating: kaggle/images/images/image_00181.jpeg  \n",
            "  inflating: kaggle/images/images/image_00182.jpeg  \n",
            "  inflating: kaggle/images/images/image_00186.jpeg  \n",
            "  inflating: kaggle/images/images/image_00191.jpeg  \n",
            "  inflating: kaggle/images/images/image_00192.jpeg  \n",
            "  inflating: kaggle/images/images/image_00193.jpeg  \n",
            "  inflating: kaggle/images/images/image_00194.jpeg  \n",
            "  inflating: kaggle/images/images/image_00195.jpeg  \n",
            "  inflating: kaggle/images/images/image_00196.jpeg  \n",
            "  inflating: kaggle/images/images/image_00197.jpeg  \n",
            "  inflating: kaggle/images/images/image_00198.jpeg  \n",
            "  inflating: kaggle/images/images/image_00202.jpeg  \n",
            "  inflating: kaggle/images/images/image_00203.jpeg  \n",
            "  inflating: kaggle/images/images/image_00204.jpeg  \n",
            "  inflating: kaggle/images/images/image_00205.jpeg  \n",
            "  inflating: kaggle/images/images/image_00206.jpeg  \n",
            "  inflating: kaggle/images/images/image_00207.jpeg  \n",
            "  inflating: kaggle/images/images/image_00209.jpeg  \n",
            "  inflating: kaggle/images/images/image_00210.jpeg  \n",
            "  inflating: kaggle/images/images/image_00211.jpeg  \n",
            "  inflating: kaggle/images/images/image_00212.jpeg  \n",
            "  inflating: kaggle/images/images/image_00213.jpeg  \n",
            "  inflating: kaggle/images/images/image_00214.jpeg  \n",
            "  inflating: kaggle/images/images/image_00215.jpeg  \n",
            "  inflating: kaggle/images/images/image_00216.jpeg  \n",
            "  inflating: kaggle/images/images/image_00217.jpeg  \n",
            "  inflating: kaggle/images/images/image_00218.jpeg  \n",
            "  inflating: kaggle/images/images/image_00219.jpeg  \n",
            "  inflating: kaggle/images/images/image_00220.jpeg  \n",
            "  inflating: kaggle/images/images/image_00221.jpeg  \n",
            "  inflating: kaggle/images/images/image_00222.jpeg  \n",
            "  inflating: kaggle/images/images/image_00223.jpeg  \n",
            "  inflating: kaggle/images/images/image_00224.jpeg  \n",
            "  inflating: kaggle/images/images/image_00225.jpeg  \n",
            "  inflating: kaggle/images/images/image_00226.jpeg  \n",
            "  inflating: kaggle/images/images/image_00227.jpeg  \n",
            "  inflating: kaggle/images/images/image_00231.jpeg  \n",
            "  inflating: kaggle/images/images/image_00237.jpeg  \n",
            "  inflating: kaggle/images/images/image_00238.jpeg  \n",
            "  inflating: kaggle/images/images/image_00239.jpeg  \n",
            "  inflating: kaggle/images/images/image_00240.jpeg  \n",
            "  inflating: kaggle/images/images/image_00244.jpeg  \n",
            "  inflating: kaggle/images/images/image_00245.jpeg  \n",
            "  inflating: kaggle/images/images/image_00246.jpeg  \n",
            "  inflating: kaggle/images/images/image_00247.jpeg  \n",
            "  inflating: kaggle/images/images/image_00248.jpeg  \n",
            "  inflating: kaggle/images/images/image_00249.jpeg  \n",
            "  inflating: kaggle/images/images/image_00250.jpeg  \n",
            "  inflating: kaggle/images/images/image_00264.jpeg  \n",
            "  inflating: kaggle/images/images/image_00265.jpeg  \n",
            "  inflating: kaggle/images/images/image_00266.jpeg  \n",
            "  inflating: kaggle/images/images/image_00267.jpeg  \n",
            "  inflating: kaggle/images/images/image_00268.jpeg  \n",
            "  inflating: kaggle/images/images/image_00269.jpeg  \n",
            "  inflating: kaggle/images/images/image_00270.jpeg  \n",
            "  inflating: kaggle/images/images/image_00271.jpeg  \n",
            "  inflating: kaggle/images/images/image_00272.jpeg  \n",
            "  inflating: kaggle/images/images/image_00273.jpeg  \n",
            "  inflating: kaggle/images/images/image_00274.jpeg  \n",
            "  inflating: kaggle/images/images/image_00275.jpeg  \n",
            "  inflating: kaggle/images/images/image_00276.jpeg  \n",
            "  inflating: kaggle/images/images/image_00284.jpeg  \n",
            "  inflating: kaggle/images/images/image_00285.jpeg  \n",
            "  inflating: kaggle/images/images/image_00286.jpeg  \n",
            "  inflating: kaggle/images/images/image_00287.jpeg  \n",
            "  inflating: kaggle/images/images/image_00288.jpeg  \n",
            "  inflating: kaggle/images/images/image_00289.jpeg  \n",
            "  inflating: kaggle/images/images/image_00291.jpeg  \n",
            "  inflating: kaggle/images/images/image_00292.jpeg  \n",
            "  inflating: kaggle/images/images/image_00293.jpeg  \n",
            "  inflating: kaggle/images/images/image_00294.jpeg  \n",
            "  inflating: kaggle/images/images/image_00295.jpeg  \n",
            "  inflating: kaggle/images/images/image_00296.jpeg  \n",
            "  inflating: kaggle/images/images/image_00301.jpeg  \n",
            "  inflating: kaggle/images/images/image_00302.jpeg  \n",
            "  inflating: kaggle/images/images/image_00308.jpeg  \n",
            "  inflating: kaggle/images/images/image_00309.jpeg  \n",
            "  inflating: kaggle/images/images/image_00310.jpeg  \n",
            "  inflating: kaggle/images/images/image_00311.jpeg  \n",
            "  inflating: kaggle/images/images/image_00312.jpeg  \n",
            "  inflating: kaggle/images/images/image_00313.jpeg  \n",
            "  inflating: kaggle/images/images/image_00314.jpeg  \n",
            "  inflating: kaggle/images/images/image_00315.jpeg  \n",
            "  inflating: kaggle/images/images/image_00316.jpeg  \n",
            "  inflating: kaggle/images/images/image_00317.jpeg  \n",
            "  inflating: kaggle/images/images/image_00318.jpeg  \n",
            "  inflating: kaggle/images/images/image_00319.jpeg  \n",
            "  inflating: kaggle/images/images/image_00328.jpeg  \n",
            "  inflating: kaggle/images/images/image_00331.jpeg  \n",
            "  inflating: kaggle/images/images/image_00332.jpeg  \n",
            "  inflating: kaggle/images/images/image_00333.jpeg  \n",
            "  inflating: kaggle/images/images/image_00334.jpeg  \n",
            "  inflating: kaggle/images/images/image_00335.jpeg  \n",
            "  inflating: kaggle/images/images/image_00336.jpeg  \n",
            "  inflating: kaggle/images/images/image_00337.jpeg  \n",
            "  inflating: kaggle/images/images/image_00338.jpeg  \n",
            "  inflating: kaggle/images/images/image_00339.jpeg  \n",
            "  inflating: kaggle/images/images/image_00341.jpeg  \n",
            "  inflating: kaggle/images/images/image_00342.jpeg  \n",
            "  inflating: kaggle/images/images/image_00343.jpeg  \n",
            "  inflating: kaggle/images/images/image_00344.jpeg  \n",
            "  inflating: kaggle/images/images/image_00345.jpeg  \n",
            "  inflating: kaggle/images/images/image_00346.jpeg  \n",
            "  inflating: kaggle/images/images/image_00347.jpeg  \n",
            "  inflating: kaggle/images/images/image_00348.jpeg  \n",
            "  inflating: kaggle/images/images/image_00349.jpeg  \n",
            "  inflating: kaggle/images/images/image_00350.jpeg  \n",
            "  inflating: kaggle/images/images/image_00364.jpeg  \n",
            "  inflating: kaggle/images/images/image_00365.jpeg  \n",
            "  inflating: kaggle/images/images/image_00366.jpeg  \n",
            "  inflating: kaggle/images/images/image_00367.jpeg  \n",
            "  inflating: kaggle/images/images/image_00368.jpeg  \n",
            "  inflating: kaggle/images/images/image_00369.jpeg  \n",
            "  inflating: kaggle/images/images/image_00370.jpeg  \n",
            "  inflating: kaggle/images/images/image_00371.jpeg  \n",
            "  inflating: kaggle/images/images/image_00372.jpeg  \n",
            "  inflating: kaggle/images/images/image_00373.jpeg  \n",
            "  inflating: kaggle/images/images/image_00374.jpeg  \n",
            "  inflating: kaggle/images/images/image_00375.jpeg  \n",
            "  inflating: kaggle/sample_submission.csv  \n",
            "  inflating: kaggle/test.csv         \n",
            "  inflating: kaggle/train.csv        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary\n",
        "!pip install -U albumentations"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T21:39:01.177578Z",
          "iopub.execute_input": "2025-02-16T21:39:01.17789Z",
          "iopub.status.idle": "2025-02-16T21:39:10.571506Z",
          "shell.execute_reply.started": "2025-02-16T21:39:01.177858Z",
          "shell.execute_reply": "2025-02-16T21:39:10.57067Z"
        },
        "id": "iLdtUGAO8Zo6",
        "outputId": "d20fadf5-d74b-432e-fa04-a4ca55917f74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.4)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.10.6)\n",
            "Requirement already satisfied: albucore==0.0.23 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.23)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.11.0.86)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (3.11.3)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.23->albumentations) (6.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (4.12.2)\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "s1vHDeXN8Zo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from torch.optim import Optimizer\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "tqdm_notebook().pandas()\n",
        "from sklearn.model_selection import train_test_split\n",
        "from multiprocessing import cpu_count\n",
        "\n",
        "import typing as ty\n",
        "from numpy.typing import NDArray\n",
        "import os\n",
        "import os.path as osp\n",
        "\n",
        "import albumentations as A\n",
        "import torchvision\n",
        "from skimage import io, transform\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "from functools import reduce\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T23:04:52.647844Z",
          "iopub.execute_input": "2025-02-11T23:04:52.648167Z",
          "iopub.status.idle": "2025-02-11T23:05:01.144756Z",
          "shell.execute_reply.started": "2025-02-11T23:04:52.648137Z",
          "shell.execute_reply": "2025-02-11T23:05:01.144077Z"
        },
        "id": "cc0kCb1M8Zo7",
        "outputId": "09b8d7b3-cd20-4895-b643-e4c3ced11b86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "f628c36e29d3410f957338bc337f399b",
            "78b8a7885a05404db5c7915a8979339b",
            "a23bf6d16be544b5a0d18b552f368f6f",
            "6fd1af769f894869b9f03d4454b84fdb",
            "636677ddefe44f73a7c2a0f82de6ce77",
            "e330acc8690e45a88243a3d9cfcb82c1",
            "8a1e4aaa42184afb874bc9753b0e9167",
            "1be12a41519040daaa20962fc958ee0a",
            "52d776a5f2524167be013412013d1df8",
            "a4c9097ef970436eadc80153f7c33253",
            "ec2f3718f69b4ef0bfbc191e295ca66d"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-df1ef1ac8c56>:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  tqdm_notebook().pandas()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f628c36e29d3410f957338bc337f399b"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "BCR8qHQ88Zo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"DATA_DIR\": \"/content/kaggle/\",\n",
        "    \"WORK_DIR\": \"/content/kaggle/working\",\n",
        "    \"IMG_DIR\": \"images/images\",\n",
        "    \"TRAIN_CSV\": \"train.csv\",\n",
        "    \"obj2id\": {\"f16\": 0, \"cougar\": 1, \"chinook\": 2, \"ah64\": 3, \"f15\": 4, \"seahawk\": 5},\n",
        "    \"id2obj\": {0: \"f16\", 1: \"cougar\", 2: \"chinook\", 3: \"ah64\", 4: \"f15\", 5: \"seahawk\"},\n",
        "    \"h_real\": 720,\n",
        "    \"w_real\": 1280,\n",
        "    \"channel\": 3,\n",
        "    \"w_resize\": 234,\n",
        "    \"h_resize\": 416,\n",
        "    \"grayscale\": False,\n",
        "}"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T23:05:13.662397Z",
          "iopub.execute_input": "2025-02-11T23:05:13.662695Z",
          "iopub.status.idle": "2025-02-11T23:05:13.667313Z",
          "shell.execute_reply.started": "2025-02-11T23:05:13.662673Z",
          "shell.execute_reply": "2025-02-11T23:05:13.666372Z"
        },
        "id": "KffYL00a8Zo8"
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(32)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using {device}')\n",
        "test = torch.ones((100, 100)).to(device)\n",
        "del test\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T23:05:19.025285Z",
          "iopub.execute_input": "2025-02-11T23:05:19.025613Z",
          "iopub.status.idle": "2025-02-11T23:05:19.276903Z",
          "shell.execute_reply.started": "2025-02-11T23:05:19.025586Z",
          "shell.execute_reply": "2025-02-11T23:05:19.276158Z"
        },
        "id": "ucvvVKWx8Zo8",
        "outputId": "57808adc-bc0e-4ce8-993d-1218baf29a85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "# Random seed"
      ],
      "metadata": {
        "trusted": true,
        "id": "k-_WOZ_I8Zo8"
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_seed():\n",
        "    random_seed = 42\n",
        "    torch.backends.cudnn.enabled = True\n",
        "    torch.manual_seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(42)\n",
        "        torch.cuda.manual_seed_all(42)\n",
        "\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "reset_seed()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T23:05:25.536658Z",
          "iopub.execute_input": "2025-02-11T23:05:25.536991Z",
          "iopub.status.idle": "2025-02-11T23:05:25.542917Z",
          "shell.execute_reply.started": "2025-02-11T23:05:25.536967Z",
          "shell.execute_reply": "2025-02-11T23:05:25.542071Z"
        },
        "id": "Lg3NznzJ8Zo9"
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to read CSV"
      ],
      "metadata": {
        "id": "iRe6w7C18Zo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_train_csv():\n",
        "    \"\"\"\n",
        "    Read the train csv file and return the dataframe with the necessary columns.\n",
        "\n",
        "    Args:\n",
        "    config: Config object\n",
        "\n",
        "    Returns:\n",
        "    df: Dataframe with the necessary columns\n",
        "\n",
        "    usage:\n",
        "    df = read_train_csv(config)\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(osp.join(config[\"DATA_DIR\"], config[\"TRAIN_CSV\"]))\n",
        "    df[\"class_id\"] = df[\"class\"].map(config[\"obj2id\"])\n",
        "    columns_f = [\"filename\", \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"class_id\"]\n",
        "    df = df[columns_f]\n",
        "    df[[\"ymin\", \"ymax\"]] = df[[\"ymin\", \"ymax\"]].div(config[\"h_real\"], axis=0)\n",
        "    df[[\"xmin\", \"xmax\"]] = df[[\"xmin\", \"xmax\"]].div(config[\"w_real\"], axis=0)\n",
        "    return df"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T23:05:29.40377Z",
          "iopub.execute_input": "2025-02-11T23:05:29.40409Z",
          "iopub.status.idle": "2025-02-11T23:05:29.409151Z",
          "shell.execute_reply.started": "2025-02-11T23:05:29.404066Z",
          "shell.execute_reply": "2025-02-11T23:05:29.408061Z"
        },
        "id": "JdIwhKLd8Zo9"
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to get the mean and standard deviation of the channels"
      ],
      "metadata": {
        "id": "vjfKWF8L8Zo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_channels_std(ds):\n",
        "    \"\"\"\n",
        "    Get the standard deviation and mean of each channel in the data.\n",
        "    \"\"\"\n",
        "    means = np.zeros(3)\n",
        "    stds = np.zeros(3)\n",
        "    n_images = 0\n",
        "\n",
        "    for x in ds:\n",
        "        img = x[\"image\"].astype(\n",
        "            np.float32\n",
        "        )  # Asegúrate de que la imagen está en float para cálculos precisos\n",
        "        n_images += 1\n",
        "\n",
        "        for channel in range(3):\n",
        "            channel_pixels = img[..., channel]\n",
        "            # Acumular la suma y suma de cuadrados para calcular la media y desviación estándar\n",
        "            means[channel] += np.mean(channel_pixels)\n",
        "            stds[channel] += np.std(channel_pixels)\n",
        "\n",
        "    # Calcular la media y desviación estándar final\n",
        "    means /= n_images\n",
        "    stds /= n_images\n",
        "\n",
        "    return means, stds"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T21:47:51.775019Z",
          "iopub.execute_input": "2025-02-16T21:47:51.775237Z",
          "iopub.status.idle": "2025-02-16T21:47:51.780329Z",
          "shell.execute_reply.started": "2025-02-16T21:47:51.775216Z",
          "shell.execute_reply": "2025-02-16T21:47:51.77955Z"
        },
        "id": "o1o-j9Jn8Zo-"
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transforms Functions"
      ],
      "metadata": {
        "id": "wRlwDVJa8Zo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ToTensor(object):\n",
        "    \"\"\"\n",
        "    Convert ndarrays in sample to Tensors for pytorch.\n",
        "\n",
        "    Arguments:\n",
        "        sample: a dictionary containing:\n",
        "            image: sample image in format (H, W, C)\n",
        "    Returns:\n",
        "        the image in (C, H, W) format.\n",
        "    \"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image = sample[\"image\"]\n",
        "\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C (0,1,2)\n",
        "        # torch image: C x H x W\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        image = torch.from_numpy(image).float()\n",
        "        sample.update({\"image\": image})\n",
        "        return sample\n",
        "\n",
        "\n",
        "class Normalizer(object):\n",
        "    \"\"\"\n",
        "    Normalize the image by subtracting the mean and dividing by the standard deviation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, stds, means):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "\n",
        "            stds: array of length 3 containing the standard deviation of each channel in RGB order.\n",
        "            means: array of length 3 containing the means of each channel in RGB order.\n",
        "        \"\"\"\n",
        "        self.stds = stds\n",
        "        self.means = means\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        \"\"\"\n",
        "        Sample: a dicitonary containing:\n",
        "            image: sample image in format (C, H, W)\n",
        "        Returns:\n",
        "            the image in (C, H, W) format with the channels normalized.\n",
        "        \"\"\"\n",
        "        image = sample[\"image\"]\n",
        "\n",
        "        for channel in range(3):\n",
        "            image[channel] = (image[channel] - self.means[channel]) / self.stds[channel]\n",
        "\n",
        "        sample[\"image\"] = image\n",
        "        return sample\n",
        "\n",
        "\n",
        "class AlbumentationsWrapper(object):\n",
        "    \"\"\"\n",
        "    Albumentations Wrapper\n",
        "\n",
        "    Arguments:\n",
        "        transform: an albumentations transform receiving an image and bounding boxes.\n",
        "\n",
        "    Returns:\n",
        "        the image transformed by the transform object.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, transform):\n",
        "        self.transform = transform\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        transformed = self.transform(\n",
        "            image=sample[\"image\"],\n",
        "            bboxes=sample[\"bbox\"],\n",
        "            # category_ids=sample['class_id']\n",
        "        )\n",
        "        sample[\"image\"] = transformed[\"image\"]\n",
        "        sample[\"bbox\"] = np.array(transformed[\"bboxes\"])\n",
        "        return sample\n",
        "\n",
        "\n",
        "def common_transforms(means, stds):\n",
        "    \"\"\"\n",
        "    Common transformations for the image.\n",
        "    Arguments:\n",
        "        means: array of length 3 containing the means of each channel in RGB order.\n",
        "        stds: array of length 3 containing the standard deviation of each channel in RGB order.\n",
        "    Returns:\n",
        "        a list of transformations.\n",
        "    \"\"\"\n",
        "    return [\n",
        "        ToTensor(),\n",
        "        Normalizer(\n",
        "            means=means,\n",
        "            stds=stds,\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "\n",
        "class TransformComposed:\n",
        "    \"\"\"\n",
        "    Compose a list of transformations.\n",
        "\n",
        "    Arguments:\n",
        "        means: array of length 3 containing the means of each channel in RGB order.\n",
        "        stds: array of length 3 containing the standard deviation of each channel in RGB order.\n",
        "\n",
        "    Returns:\n",
        "        a composed transformation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, means, stds):\n",
        "        self.means = means\n",
        "        self.stds = stds\n",
        "\n",
        "    def getTransform(self, transforms=[]):\n",
        "        return torchvision.transforms.Compose(\n",
        "            [AlbumentationsWrapper(t) for t in transforms]\n",
        "            + common_transforms(self.means, self.stds)\n",
        "        )"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T00:03:21.717662Z",
          "iopub.execute_input": "2025-02-11T00:03:21.71802Z",
          "iopub.status.idle": "2025-02-11T00:03:21.727727Z",
          "shell.execute_reply.started": "2025-02-11T00:03:21.71799Z",
          "shell.execute_reply": "2025-02-11T00:03:21.72671Z"
        },
        "id": "qKjOYM538Zo_"
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Required functions to train a pytorch model"
      ],
      "metadata": {
        "id": "ueR3X7SZ8ZpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class to load dataset"
      ],
      "metadata": {
        "id": "Mc8LKn9i8ZpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_func_inp_signature = ty.Dict[str, NDArray[np.float_]]\n",
        "transform_func_signature = ty.Callable[\n",
        "    [transform_func_inp_signature],\n",
        "    transform_func_inp_signature\n",
        "]\n",
        "\n",
        "class militarDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Location image dataset\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        df: pd.DataFrame,\n",
        "        root_dir: str,\n",
        "        labeled: bool = True,\n",
        "        transform: ty.Optional[ty.List[transform_func_signature]] = None,\n",
        "        output_size: ty.Optional[tuple] = None  # Añadir parámetro para tamaño de salida\n",
        "    ) -> None:\n",
        "        self.df = df\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.labeled = labeled\n",
        "        self.output_size = output_size  # Almacenar el tamaño de salida\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx: int) -> transform_func_signature:\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # Read image\n",
        "        img_name = os.path.join(self.root_dir, self.df.filename.iloc[idx])\n",
        "        #img_name = os.path.join(self.root_dir, self.df.iloc[idx]['filename'])\n",
        "        image = io.imread(img_name)\n",
        "        #image = cv2.imread(img_name)\n",
        "\n",
        "\n",
        "        #print(f\"Dimensiones originales de la imagen: {image.shape}\")  # Agregar para depuración\n",
        "        if image is None:\n",
        "            raise FileNotFoundError(f\"Image not found: {img_name}\")\n",
        "\n",
        "        if image.ndim == 2:  # Si la imagen está en escala de grises\n",
        "            image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)  # Convertir a RGB\n",
        "        elif image.shape[2] == 4:  # Si la imagen es RGBA\n",
        "            image = image[:, :, :3]\n",
        "\n",
        "        # Redimensionar la imagen si se especifica un tamaño de salida\n",
        "        if self.output_size:\n",
        "            image = cv2.resize(image, self.output_size)  # Redimensionar la imagen\n",
        "\n",
        "        sample = {'image': image}\n",
        "\n",
        "        if self.labeled:\n",
        "            # Read labels\n",
        "            img_class = self.df.class_id.iloc[idx]\n",
        "            img_bbox = self.df.iloc[idx, 1:5]\n",
        "\n",
        "            img_bbox = np.array([img_bbox]).astype('float')\n",
        "            img_class = np.array([img_class]).astype('int')\n",
        "            sample.update({'bbox': img_bbox, 'class_id': img_class})\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T00:03:31.425585Z",
          "iopub.execute_input": "2025-02-11T00:03:31.42611Z",
          "iopub.status.idle": "2025-02-11T00:03:31.435591Z",
          "shell.execute_reply.started": "2025-02-11T00:03:31.426066Z",
          "shell.execute_reply": "2025-02-11T00:03:31.434716Z"
        },
        "id": "HXosgGPF8ZpA"
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "id": "lyF6Kvg18ZpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def iou(y_true: Tensor, y_pred: Tensor):\n",
        "    pairwise_iou = torchvision.ops.box_iou(y_true.squeeze(), y_pred.squeeze())\n",
        "    result = torch.trace(pairwise_iou) / pairwise_iou.size()[0]\n",
        "    return result\n",
        "\n",
        "def accuracy(y_true: Tensor, y_pred: Tensor):\n",
        "    pred = torch.argmax(y_pred, axis=-1)\n",
        "    y_true = y_true.squeeze()\n",
        "    correct = torch.eq(pred, y_true).float()\n",
        "    total = torch.ones_like(correct)\n",
        "    result = torch.divide(torch.sum(correct), torch.sum(total))\n",
        "    return result"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T00:03:40.455424Z",
          "iopub.execute_input": "2025-02-11T00:03:40.455702Z",
          "iopub.status.idle": "2025-02-11T00:03:40.460602Z",
          "shell.execute_reply.started": "2025-02-11T00:03:40.455682Z",
          "shell.execute_reply": "2025-02-11T00:03:40.459737Z"
        },
        "id": "ognRX6aN8ZpB"
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss function"
      ],
      "metadata": {
        "id": "-jzOA4mv8ZpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(y_true, y_preds, alpha: float = 0.5):\n",
        "    cls_y_true, cls_y_pred = y_true['class_id'].long(), y_preds['class_id'].float().unsqueeze(-1)\n",
        "    reg_y_true, reg_y_pred = y_true['bbox'].float().squeeze(), y_preds['bbox'].float().squeeze()\n",
        "\n",
        "    cls_loss = F.cross_entropy(cls_y_pred, cls_y_true)\n",
        "\n",
        "    reg_loss = F.mse_loss(reg_y_pred, reg_y_true)\n",
        "    # Adds weights to both tasks\n",
        "    total_loss = (1 - alpha) * cls_loss + alpha * reg_loss\n",
        "    return dict(loss=total_loss, reg_loss=reg_loss,cls_loss=cls_loss)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T00:03:45.531399Z",
          "iopub.execute_input": "2025-02-11T00:03:45.531733Z",
          "iopub.status.idle": "2025-02-11T00:03:45.537Z",
          "shell.execute_reply.started": "2025-02-11T00:03:45.531708Z",
          "shell.execute_reply": "2025-02-11T00:03:45.535973Z"
        },
        "id": "wsKf9IrQ8ZpC"
      },
      "outputs": [],
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "source": [
        "## callbacks"
      ],
      "metadata": {
        "id": "Pd7AWhvF8ZpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def printer(logs: ty.Dict[str, ty.Any]):\n",
        "    # print every 10 steps\n",
        "    if logs['iters'] % 10 != 0:\n",
        "        return\n",
        "    print('Iteration #: ',logs['iters'])\n",
        "    for name, value in logs.items():\n",
        "        if name == 'iters':\n",
        "            continue\n",
        "\n",
        "        if type(value) in [float, int]:\n",
        "            value = round(value, 4)\n",
        "        elif type(value) is torch.Tensor:\n",
        "            value = torch.round(value, decimals=4)\n",
        "\n",
        "        print(f'\\t{name} = {value}')\n",
        "    print()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T00:03:48.264864Z",
          "iopub.execute_input": "2025-02-11T00:03:48.26531Z",
          "iopub.status.idle": "2025-02-11T00:03:48.270146Z",
          "shell.execute_reply.started": "2025-02-11T00:03:48.265279Z",
          "shell.execute_reply": "2025-02-11T00:03:48.269247Z"
        },
        "id": "QdAe_xb-8ZpC"
      },
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define model"
      ],
      "metadata": {
        "id": "1Xe1askX8ZpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_output_shape(\n",
        "    model: nn.Sequential, image_dim: ty.Tuple[int, int, int], device: str = \"cpu\"\n",
        ") -> ty.Tuple[int, int, int]:\n",
        "    return model(torch.rand(*(image_dim)).to(device)).data.shape\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        backbone: nn.Module,\n",
        "        input_shape: ty.Tuple[int, int, int] = (3, 255, 400),\n",
        "        n_classes: int = 6,\n",
        "        device: str = \"cpu\"\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Model with one input (image) and two outputs:\n",
        "            1. Digit classification (classification).\n",
        "            2. Bounding box prediction (regression).\n",
        "\n",
        "        Arguments:\n",
        "            input_shape: input shape of the image in format (C, H, W)\n",
        "            n_classes: number of classes to perfrom classification with\n",
        "            backbone: Initial model to extract features from the image and pass to clasification and regresion heads.\n",
        "\n",
        "        Attributes:\n",
        "            backbone: ConvNet that process the image and\n",
        "            returns a flattened vector with the information of the\n",
        "            activations.\n",
        "\n",
        "            cls_head: MLP that receives the flattened input from the backbone\n",
        "            and predicts the classification logits for the classes (classficiation task).\n",
        "\n",
        "            reg_head: MLP that receives the flattened input from the backbone\n",
        "            and predicts the coordinates of the predicted bounding box (regression task).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_shape = input_shape\n",
        "\n",
        "        # When doing transfer learning, use pretrained model instead of custom backbone\n",
        "        self.backbone = backbone\n",
        "\n",
        "        backbone_output_shape = get_output_shape(self.backbone, [1, *input_shape],device)\n",
        "        backbone_output_features = reduce(lambda x, y: x * y, backbone_output_shape)\n",
        "\n",
        "        self.cls_head = nn.Sequential(\n",
        "            nn.Linear(in_features=backbone_output_features, out_features=768),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(768, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, n_classes),\n",
        "        )\n",
        "\n",
        "        self.reg_head = nn.Sequential(\n",
        "            nn.Linear(in_features=backbone_output_features, out_features=768),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(768, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 4),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> ty.Dict[str, Tensor]:\n",
        "        features = self.backbone(x)\n",
        "        cls_logits = self.cls_head(features)\n",
        "        pred_bbox = self.reg_head(features)\n",
        "        predictions = {\"bbox\": pred_bbox, \"class_id\": cls_logits}\n",
        "        return predictions"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T00:03:56.219673Z",
          "iopub.execute_input": "2025-02-11T00:03:56.219976Z",
          "iopub.status.idle": "2025-02-11T00:03:56.227561Z",
          "shell.execute_reply.started": "2025-02-11T00:03:56.219954Z",
          "shell.execute_reply": "2025-02-11T00:03:56.226693Z"
        },
        "id": "bjyZV7MB8ZpD"
      },
      "outputs": [],
      "execution_count": 18
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to train the model"
      ],
      "metadata": {
        "id": "HmanJ0LZ8ZpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(\n",
        "    logs: ty.Dict[str, ty.Any],\n",
        "    labels: ty.Dict[str, Tensor],\n",
        "    preds: ty.Dict[str, Tensor],\n",
        "    eval_set: str,\n",
        "    metrics: ty.Dict[str, ty.Callable[[Tensor, Tensor], Tensor]],\n",
        "    losses: ty.Optional[ty.Dict[str, Tensor]] = None,\n",
        ") -> ty.Dict[str, ty.Any]:\n",
        "\n",
        "    if losses is not None:\n",
        "        for loss_name, loss_value in losses.items():\n",
        "            logs[f'{eval_set}_{loss_name}'] = loss_value\n",
        "\n",
        "    for task_name, label in labels.items():\n",
        "        for metric_name, metric in metrics[task_name]:\n",
        "            value = metric(label, preds[task_name])\n",
        "            logs[f'{eval_set}_{metric_name}'] = value\n",
        "\n",
        "    return logs\n",
        "\n",
        "def step(\n",
        "    model: Model,\n",
        "    optimizer: Optimizer,\n",
        "    batch: militarDataset,\n",
        "    loss_fn: ty.Callable[[ty.Dict[str, torch.Tensor]], torch.Tensor],\n",
        "    device: str,\n",
        "    train: bool = False,\n",
        ") -> ty.Tuple[ty.Dict[str, Tensor], ty.Dict[str, Tensor]]:\n",
        "\n",
        "    if train:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    #img = batch['image'].to(device)\n",
        "    img = batch.pop('image').to(device)\n",
        "\n",
        "    for k in list(batch.keys()):\n",
        "        batch[k] = batch[k].to(device)\n",
        "\n",
        "    preds = model(img.float())\n",
        "    losses = loss_fn(batch, preds)\n",
        "    final_loss = losses['loss']\n",
        "\n",
        "    if train:\n",
        "        final_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return losses, preds\n",
        "\n",
        "\n",
        "def train(\n",
        "    model: Model,\n",
        "    optimizer: Optimizer,\n",
        "    dataset: DataLoader,\n",
        "    eval_datasets: ty.List[ty.Tuple[str, DataLoader]],\n",
        "    loss_fn: ty.Callable[[ty.Dict[str, torch.Tensor]], torch.Tensor],\n",
        "    metrics: ty.Dict[str, ty.Callable[[Tensor, Tensor], Tensor]],\n",
        "    callbacks: ty.List[ty.Callable[[ty.Dict[ty.Any, ty.Any]], None]],\n",
        "    device: str,\n",
        "    train_steps: 100,\n",
        "    eval_steps: 10,\n",
        ") -> Model:\n",
        "    # Send model to device (GPU or CPU)\n",
        "    model = model.to(device)\n",
        "    iters = 0\n",
        "    iterator = iter(dataset)\n",
        "    assert train_steps > eval_steps, 'Train steps should be greater than the eval steps'\n",
        "\n",
        "    while iters <= train_steps:\n",
        "        logs = dict()\n",
        "        logs['iters'] = iters\n",
        "        try:\n",
        "            batch = next(iterator)\n",
        "        except StopIteration:\n",
        "            iterator = iter(dataset)\n",
        "            batch = next(iterator)\n",
        "        # Send batch to device\n",
        "        losses, preds = step(model, optimizer, batch, loss_fn, device, train=True)\n",
        "        logs = evaluate(logs, batch, preds, 'train', metrics, losses)\n",
        "\n",
        "        # Eval every eval_steps iterations\n",
        "        if iters % eval_steps == 0:\n",
        "            # Evaluate\n",
        "            # Deactives layers that only needed to train\n",
        "            # https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615\n",
        "            model.eval()\n",
        "\n",
        "            # Avoids calculating gradients in evaluation dataset.\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for name, dataset in eval_datasets:\n",
        "\n",
        "                    for batch in dataset:\n",
        "                        losses, preds = step(model, optimizer, batch, loss_fn, device, train=False)\n",
        "                        logs = evaluate(logs, batch, preds, name, metrics, losses)\n",
        "\n",
        "        for callback in callbacks:\n",
        "            callback(logs)\n",
        "\n",
        "        iters += 1\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T00:04:01.337541Z",
          "iopub.execute_input": "2025-02-11T00:04:01.337858Z",
          "iopub.status.idle": "2025-02-11T00:04:01.34942Z",
          "shell.execute_reply.started": "2025-02-11T00:04:01.337832Z",
          "shell.execute_reply": "2025-02-11T00:04:01.348164Z"
        },
        "id": "-e_qtW8X8ZpD"
      },
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split train and validation"
      ],
      "metadata": {
        "id": "HlLmqg028ZpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seed()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T00:04:06.751342Z",
          "iopub.execute_input": "2025-02-11T00:04:06.751629Z",
          "iopub.status.idle": "2025-02-11T00:04:06.756258Z",
          "shell.execute_reply.started": "2025-02-11T00:04:06.751608Z",
          "shell.execute_reply": "2025-02-11T00:04:06.75555Z"
        },
        "id": "vTRyhA-U8ZpE"
      },
      "outputs": [],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": [
        "df = read_train_csv()\n",
        "train_df, val_df = train_test_split(\n",
        "    df, stratify=df[\"class_id\"], test_size=0.25, random_state=42\n",
        ")\n",
        "print(f'training set shape: {train_df.shape}')\n",
        "print(f'validation set shape: {val_df.shape}')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T00:04:11.457478Z",
          "iopub.execute_input": "2025-02-11T00:04:11.457829Z",
          "iopub.status.idle": "2025-02-11T00:04:11.494624Z",
          "shell.execute_reply.started": "2025-02-11T00:04:11.457801Z",
          "shell.execute_reply": "2025-02-11T00:04:11.493884Z"
        },
        "id": "g4E2E-xd8ZpF",
        "outputId": "ba86d45c-d16e-43b1-bead-05cfed1cc926",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training set shape: (141, 6)\n",
            "validation set shape: (48, 6)\n"
          ]
        }
      ],
      "execution_count": 21
    },
    {
      "cell_type": "code",
      "source": [
        "#Se crean cuatro listas vacías para almacenar:\n",
        "list_image = list(df.filename)\n",
        "\n",
        "#Este bucle itera sobre cada nombre de archivo en la lista list_image. tqdm es una librería que muestra una barra de progreso para visualizar el avance del bucle\n",
        "\n",
        "for i in tqdm(list_image): ## tqdm(list_image)dura 40 segundos\n",
        "    ruta_imagen = osp.join(config[\"DATA_DIR\"], config[\"IMG_DIR\"], i)\n",
        "    imagen = io.imread(ruta_imagen)\n",
        "    shapes = imagen.shape\n",
        "    dimen = imagen.ndim\n",
        "    imagen = Image.open(ruta_imagen)\n",
        "    w, h = imagen.size\n",
        "    imagen = imagen.resize((w, h), Image.Resampling.LANCZOS)\n",
        "    imagen.save(ruta_imagen)\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "gOj739zvEhRP",
        "outputId": "5f349e8a-488d-449f-9f80-6df9b31aac1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 189/189 [00:08<00:00, 23.46it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define transforms"
      ],
      "metadata": {
        "id": "MCyg8hbY8ZpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = osp.join(config[\"DATA_DIR\"], config[\"IMG_DIR\"])\n",
        "train_ds = militarDataset(train_df, root_dir)\n",
        "means, stds = get_channels_std(train_ds)\n",
        "common_transforms_I = common_transforms(means,stds)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T00:04:49.448538Z",
          "iopub.execute_input": "2025-02-11T00:04:49.448829Z",
          "iopub.status.idle": "2025-02-11T00:04:53.239595Z",
          "shell.execute_reply.started": "2025-02-11T00:04:49.448808Z",
          "shell.execute_reply": "2025-02-11T00:04:53.238937Z"
        },
        "id": "Z9JyAvju8ZpL"
      },
      "outputs": [],
      "execution_count": 23
    },
    {
      "cell_type": "code",
      "source": [
        "bbox_params = A.BboxParams(format=\"albumentations\", label_fields=[])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T00:04:58.509359Z",
          "iopub.execute_input": "2025-02-11T00:04:58.509656Z",
          "iopub.status.idle": "2025-02-11T00:04:58.513816Z",
          "shell.execute_reply.started": "2025-02-11T00:04:58.509633Z",
          "shell.execute_reply": "2025-02-11T00:04:58.512819Z"
        },
        "id": "s-TqdtqT8ZpL"
      },
      "outputs": [],
      "execution_count": 24
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_augmentations = A.Compose([\n",
        "    A.HorizontalFlip(p=0),\n",
        "    A.Rotate(limit=45, p=0),\n",
        "    A.AutoContrast(p=0),\n",
        "    A.Defocus(p=0),\n",
        "    A.Downscale(p=0),\n",
        "    A.GaussNoise(p=0),\n",
        "    A.GaussianBlur(p=0),\n",
        "    A.HueSaturationValue(p=0),\n",
        "    A.ISONoise(p=0),\n",
        "    A.PlanckianJitter(p=0),\n",
        "    A.PlasmaShadow(p=0.5),\n",
        "    A.Posterize(p=0),\n",
        "    A.RandomFog(p=0),\n",
        "    A.RandomSnow(p=0),\n",
        "    A.RandomSunFlare(p=0),\n",
        "    A.SaltAndPepper(p=0),\n",
        "    A.Sharpen(p=0),\n",
        "    A.ZoomBlur(p=0)\n",
        "    ],\n",
        "    bbox_params=bbox_params\n",
        ")\n",
        "\n",
        "train_transforms = torchvision.transforms.Compose(\n",
        "    [\n",
        "        AlbumentationsWrapper(train_data_augmentations),\n",
        "    ] + common_transforms_I\n",
        ")\n",
        "\n",
        "eval_transforms = torchvision.transforms.Compose(common_transforms_I)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T00:05:02.679176Z",
          "iopub.execute_input": "2025-02-11T00:05:02.679462Z",
          "iopub.status.idle": "2025-02-11T00:05:02.693415Z",
          "shell.execute_reply.started": "2025-02-11T00:05:02.679442Z",
          "shell.execute_reply": "2025-02-11T00:05:02.692477Z"
        },
        "id": "5WvrkgW68ZpM"
      },
      "outputs": [],
      "execution_count": 25
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning"
      ],
      "metadata": {
        "id": "hKzmPd9w8ZpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        # Extract VGG-16 Feature Layers\n",
        "        self.features = list(model.features)\n",
        "        self.features = nn.Sequential(*self.features)\n",
        "        # Extract VGG-16 Average Pooling Layer\n",
        "        self.pooling = model.avgpool\n",
        "        # Convert the image into one-dimensional vector\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # It will take the input 'x' until it returns the feature vector called 'out'\n",
        "        out = self.features(x)\n",
        "        out = self.pooling(out)\n",
        "        out = self.flatten(out)\n",
        "        out = self.dropout(out)\n",
        "        return out"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T00:05:07.371572Z",
          "iopub.execute_input": "2025-02-11T00:05:07.37187Z",
          "iopub.status.idle": "2025-02-11T00:05:07.377365Z",
          "shell.execute_reply.started": "2025-02-11T00:05:07.371848Z",
          "shell.execute_reply": "2025-02-11T00:05:07.376351Z"
        },
        "id": "s5lWXWyE8ZpM"
      },
      "outputs": [],
      "execution_count": 26
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vgg16"
      ],
      "metadata": {
        "id": "yonRJpLZ8ZpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import vgg16, VGG16_Weights\n",
        "\n",
        "\n",
        "# Load the vgg16 model\n",
        "vgg16_model = vgg16(weights=VGG16_Weights.DEFAULT, progress=True)\n",
        "pretrained_model = FeatureExtractor(vgg16_model).to(device)\n",
        "pretrained_model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T00:05:11.187067Z",
          "iopub.execute_input": "2025-02-11T00:05:11.18745Z",
          "iopub.status.idle": "2025-02-11T00:05:15.711461Z",
          "shell.execute_reply.started": "2025-02-11T00:05:11.187422Z",
          "shell.execute_reply": "2025-02-11T00:05:15.710772Z"
        },
        "id": "EDHszTNX8ZpT",
        "outputId": "adf19ccb-866b-4a80-bc6b-253e87d09bf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:07<00:00, 74.1MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FeatureExtractor(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (pooling): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "execution_count": 27
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EfficientNetV2"
      ],
      "metadata": {
        "id": "NePiG62U8ZpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchvision.models import efficientnet_v2_l, EfficientNet_V2_L_Weights\n",
        "\n",
        "# Load the efficient net v2 model\n",
        "# env2_model = efficientnet_v2_l(weights=EfficientNet_V2_L_Weights.DEFAULT)\n",
        "# pretrained_model = FeatureExtractor(env2_model).to(device)\n",
        "# pretrained_model"
      ],
      "metadata": {
        "trusted": true,
        "id": "jT8jCLWZ8ZpT"
      },
      "outputs": [],
      "execution_count": 28
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize model"
      ],
      "metadata": {
        "id": "n-43ASG08ZpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds[0]['image'].shape"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T00:05:25.326121Z",
          "iopub.execute_input": "2025-02-11T00:05:25.326404Z",
          "iopub.status.idle": "2025-02-11T00:05:25.340717Z",
          "shell.execute_reply.started": "2025-02-11T00:05:25.326384Z",
          "shell.execute_reply": "2025-02-11T00:05:25.33995Z"
        },
        "id": "oJmTJq8r8ZpU",
        "outputId": "ed356203-698c-42d0-8f0b-d56e4310420b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(720, 1280, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "execution_count": 29
    },
    {
      "cell_type": "code",
      "source": [
        "# Hparams\n",
        "batch_size = 32\n",
        "lr = 0.001\n",
        "w = 256\n",
        "h = 455\n",
        "c = 3\n",
        "\n",
        "# Data\n",
        "train_ds = militarDataset(train_df, root_dir=root_dir, transform=train_transforms,output_size=(w,h))\n",
        "val_ds = militarDataset(val_df, root_dir=root_dir, transform=eval_transforms,output_size=(w,h))\n",
        "\n",
        "train_data = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=cpu_count())\n",
        "val_data = DataLoader(val_ds, batch_size=batch_size, num_workers=cpu_count())\n",
        "\n",
        "# Model\n",
        "model = Model(pretrained_model,(c,h,w),device=device).to(device)\n",
        "summary(model, model.input_shape)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(lr=lr, params=model.parameters())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T00:05:49.208581Z",
          "iopub.execute_input": "2025-02-11T00:05:49.208879Z",
          "iopub.status.idle": "2025-02-11T00:05:50.184965Z",
          "shell.execute_reply.started": "2025-02-11T00:05:49.208858Z",
          "shell.execute_reply": "2025-02-11T00:05:50.184149Z"
        },
        "id": "Ryt9qLc98ZpU",
        "outputId": "22bc1242-8132-44f1-e460-0feb8d35dea7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 455, 256]           1,792\n",
            "              ReLU-2         [-1, 64, 455, 256]               0\n",
            "            Conv2d-3         [-1, 64, 455, 256]          36,928\n",
            "              ReLU-4         [-1, 64, 455, 256]               0\n",
            "         MaxPool2d-5         [-1, 64, 227, 128]               0\n",
            "            Conv2d-6        [-1, 128, 227, 128]          73,856\n",
            "              ReLU-7        [-1, 128, 227, 128]               0\n",
            "            Conv2d-8        [-1, 128, 227, 128]         147,584\n",
            "              ReLU-9        [-1, 128, 227, 128]               0\n",
            "        MaxPool2d-10         [-1, 128, 113, 64]               0\n",
            "           Conv2d-11         [-1, 256, 113, 64]         295,168\n",
            "             ReLU-12         [-1, 256, 113, 64]               0\n",
            "           Conv2d-13         [-1, 256, 113, 64]         590,080\n",
            "             ReLU-14         [-1, 256, 113, 64]               0\n",
            "           Conv2d-15         [-1, 256, 113, 64]         590,080\n",
            "             ReLU-16         [-1, 256, 113, 64]               0\n",
            "        MaxPool2d-17          [-1, 256, 56, 32]               0\n",
            "           Conv2d-18          [-1, 512, 56, 32]       1,180,160\n",
            "             ReLU-19          [-1, 512, 56, 32]               0\n",
            "           Conv2d-20          [-1, 512, 56, 32]       2,359,808\n",
            "             ReLU-21          [-1, 512, 56, 32]               0\n",
            "           Conv2d-22          [-1, 512, 56, 32]       2,359,808\n",
            "             ReLU-23          [-1, 512, 56, 32]               0\n",
            "        MaxPool2d-24          [-1, 512, 28, 16]               0\n",
            "           Conv2d-25          [-1, 512, 28, 16]       2,359,808\n",
            "             ReLU-26          [-1, 512, 28, 16]               0\n",
            "           Conv2d-27          [-1, 512, 28, 16]       2,359,808\n",
            "             ReLU-28          [-1, 512, 28, 16]               0\n",
            "           Conv2d-29          [-1, 512, 28, 16]       2,359,808\n",
            "             ReLU-30          [-1, 512, 28, 16]               0\n",
            "        MaxPool2d-31           [-1, 512, 14, 8]               0\n",
            "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
            "          Flatten-33                [-1, 25088]               0\n",
            "          Dropout-34                [-1, 25088]               0\n",
            " FeatureExtractor-35                [-1, 25088]               0\n",
            "           Linear-36                  [-1, 768]      19,268,352\n",
            "             ReLU-37                  [-1, 768]               0\n",
            "           Linear-38                  [-1, 256]         196,864\n",
            "             ReLU-39                  [-1, 256]               0\n",
            "          Dropout-40                  [-1, 256]               0\n",
            "           Linear-41                    [-1, 6]           1,542\n",
            "           Linear-42                  [-1, 768]      19,268,352\n",
            "             ReLU-43                  [-1, 768]               0\n",
            "           Linear-44                  [-1, 256]         196,864\n",
            "             ReLU-45                  [-1, 256]               0\n",
            "           Linear-46                  [-1, 128]          32,896\n",
            "             ReLU-47                  [-1, 128]               0\n",
            "          Dropout-48                  [-1, 128]               0\n",
            "           Linear-49                    [-1, 4]             516\n",
            "================================================================\n",
            "Total params: 53,680,074\n",
            "Trainable params: 53,680,074\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.33\n",
            "Forward/backward pass size (MB): 505.99\n",
            "Params size (MB): 204.77\n",
            "Estimated Total Size (MB): 712.10\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "execution_count": 30
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "o71xTOaQ8ZpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = train(\n",
        "    model,\n",
        "    optimizer,\n",
        "    train_data,\n",
        "    eval_datasets=[('val', val_data)],\n",
        "    loss_fn=loss_fn,\n",
        "    metrics={\n",
        "        'bbox': [('iou', iou)],\n",
        "        'class_id': [('accuracy', accuracy)]\n",
        "    },\n",
        "    callbacks=[printer],\n",
        "    device=device,\n",
        "    train_steps=130,\n",
        "    eval_steps=10\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T00:05:55.601311Z",
          "iopub.execute_input": "2025-02-11T00:05:55.601603Z",
          "iopub.status.idle": "2025-02-11T00:07:00.34214Z",
          "shell.execute_reply.started": "2025-02-11T00:05:55.601581Z",
          "shell.execute_reply": "2025-02-11T00:07:00.341136Z"
        },
        "id": "fTvBt-Kk8ZpU",
        "outputId": "68e36424-8bca-4402-f556-301e41ccc1e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration #:  0\n",
            "\ttrain_loss = 1.0384999513626099\n",
            "\ttrain_reg_loss = 0.265500009059906\n",
            "\ttrain_cls_loss = 1.811400055885315\n",
            "\ttrain_iou = 0.0001\n",
            "\ttrain_accuracy = 0.09380000084638596\n",
            "\tval_loss = 1.0160000324249268\n",
            "\tval_reg_loss = 0.09350000321865082\n",
            "\tval_cls_loss = 1.938599944114685\n",
            "\tval_iou = 0.1789\n",
            "\tval_accuracy = 0.125\n",
            "\n",
            "Iteration #:  10\n",
            "\ttrain_loss = 0.9714999794960022\n",
            "\ttrain_reg_loss = 0.17890000343322754\n",
            "\ttrain_cls_loss = 1.7640999555587769\n",
            "\ttrain_iou = 0.0003\n",
            "\ttrain_accuracy = 0.1875\n",
            "\tval_loss = 0.9976000189781189\n",
            "\tval_reg_loss = 0.21119999885559082\n",
            "\tval_cls_loss = 1.784000039100647\n",
            "\tval_iou = 0.0\n",
            "\tval_accuracy = 0.1875\n",
            "\n",
            "Iteration #:  20\n",
            "\ttrain_loss = 0.9157000184059143\n",
            "\ttrain_reg_loss = 0.4153999984264374\n",
            "\ttrain_cls_loss = 1.4158999919891357\n",
            "\ttrain_iou = 0.0313\n",
            "\ttrain_accuracy = 0.5\n",
            "\tval_loss = 0.9010999798774719\n",
            "\tval_reg_loss = 0.13449999690055847\n",
            "\tval_cls_loss = 1.6677000522613525\n",
            "\tval_iou = 0.0074\n",
            "\tval_accuracy = 0.5\n",
            "\n",
            "Iteration #:  30\n",
            "\ttrain_loss = 0.3720000088214874\n",
            "\ttrain_reg_loss = 0.04230000078678131\n",
            "\ttrain_cls_loss = 0.7017999887466431\n",
            "\ttrain_iou = 0.2127\n",
            "\ttrain_accuracy = 0.75\n",
            "\tval_loss = 0.21359999477863312\n",
            "\tval_reg_loss = 0.029999999329447746\n",
            "\tval_cls_loss = 0.39719998836517334\n",
            "\tval_iou = 0.2787\n",
            "\tval_accuracy = 0.875\n",
            "\n",
            "Iteration #:  40\n",
            "\ttrain_loss = 0.09480000287294388\n",
            "\ttrain_reg_loss = 0.05660000070929527\n",
            "\ttrain_cls_loss = 0.13300000131130219\n",
            "\ttrain_iou = 0.0997\n",
            "\ttrain_accuracy = 1.0\n",
            "\tval_loss = 0.11900000274181366\n",
            "\tval_reg_loss = 0.02800000086426735\n",
            "\tval_cls_loss = 0.20999999344348907\n",
            "\tval_iou = 0.2544\n",
            "\tval_accuracy = 0.9375\n",
            "\n",
            "Iteration #:  50\n",
            "\ttrain_loss = 0.03350000083446503\n",
            "\ttrain_reg_loss = 0.014700000174343586\n",
            "\ttrain_cls_loss = 0.052299998700618744\n",
            "\ttrain_iou = 0.3121\n",
            "\ttrain_accuracy = 0.9375\n",
            "\tval_loss = 0.008299999870359898\n",
            "\tval_reg_loss = 0.011900000274181366\n",
            "\tval_cls_loss = 0.004600000102072954\n",
            "\tval_iou = 0.3549\n",
            "\tval_accuracy = 1.0\n",
            "\n",
            "Iteration #:  60\n",
            "\ttrain_loss = 0.007000000216066837\n",
            "\ttrain_reg_loss = 0.01209999993443489\n",
            "\ttrain_cls_loss = 0.0019000000320374966\n",
            "\ttrain_iou = 0.2984\n",
            "\ttrain_accuracy = 1.0\n",
            "\tval_loss = 0.004100000020116568\n",
            "\tval_reg_loss = 0.0071000000461936\n",
            "\tval_cls_loss = 0.0012000000569969416\n",
            "\tval_iou = 0.3989\n",
            "\tval_accuracy = 1.0\n",
            "\n",
            "Iteration #:  70\n",
            "\ttrain_loss = 0.0012000000569969416\n",
            "\ttrain_reg_loss = 0.002199999988079071\n",
            "\ttrain_cls_loss = 0.00019999999494757503\n",
            "\ttrain_iou = 0.5817\n",
            "\ttrain_accuracy = 1.0\n",
            "\tval_loss = 0.00139999995008111\n",
            "\tval_reg_loss = 0.0026000000070780516\n",
            "\tval_cls_loss = 0.00019999999494757503\n",
            "\tval_iou = 0.5493\n",
            "\tval_accuracy = 1.0\n",
            "\n",
            "Iteration #:  80\n",
            "\ttrain_loss = 0.00039999998989515007\n",
            "\ttrain_reg_loss = 0.0007999999797903001\n",
            "\ttrain_cls_loss = 0.0\n",
            "\ttrain_iou = 0.7222\n",
            "\ttrain_accuracy = 1.0\n",
            "\tval_loss = 0.0003000000142492354\n",
            "\tval_reg_loss = 0.0006000000284984708\n",
            "\tval_cls_loss = 0.0\n",
            "\tval_iou = 0.7574\n",
            "\tval_accuracy = 1.0\n",
            "\n",
            "Iteration #:  90\n",
            "\ttrain_loss = 0.00019999999494757503\n",
            "\ttrain_reg_loss = 0.00039999998989515007\n",
            "\ttrain_cls_loss = 0.0\n",
            "\ttrain_iou = 0.8013\n",
            "\ttrain_accuracy = 1.0\n",
            "\tval_loss = 0.00019999999494757503\n",
            "\tval_reg_loss = 0.0003000000142492354\n",
            "\tval_cls_loss = 0.0\n",
            "\tval_iou = 0.8083\n",
            "\tval_accuracy = 1.0\n",
            "\n",
            "Iteration #:  100\n",
            "\ttrain_loss = 9.999999747378752e-05\n",
            "\ttrain_reg_loss = 9.999999747378752e-05\n",
            "\ttrain_cls_loss = 0.0\n",
            "\ttrain_iou = 0.8906\n",
            "\ttrain_accuracy = 1.0\n",
            "\tval_loss = 9.999999747378752e-05\n",
            "\tval_reg_loss = 9.999999747378752e-05\n",
            "\tval_cls_loss = 0.0\n",
            "\tval_iou = 0.8914\n",
            "\tval_accuracy = 1.0\n",
            "\n",
            "Iteration #:  110\n",
            "\ttrain_loss = 0.0\n",
            "\ttrain_reg_loss = 0.0\n",
            "\ttrain_cls_loss = 0.0\n",
            "\ttrain_iou = 0.9362\n",
            "\ttrain_accuracy = 1.0\n",
            "\tval_loss = 0.0\n",
            "\tval_reg_loss = 0.0\n",
            "\tval_cls_loss = 0.0\n",
            "\tval_iou = 0.9233\n",
            "\tval_accuracy = 1.0\n",
            "\n",
            "Iteration #:  120\n",
            "\ttrain_loss = 0.0\n",
            "\ttrain_reg_loss = 0.0\n",
            "\ttrain_cls_loss = 0.0\n",
            "\ttrain_iou = 0.9622\n",
            "\ttrain_accuracy = 1.0\n",
            "\tval_loss = 0.0\n",
            "\tval_reg_loss = 0.0\n",
            "\tval_cls_loss = 0.0\n",
            "\tval_iou = 0.9694\n",
            "\tval_accuracy = 1.0\n",
            "\n",
            "Iteration #:  130\n",
            "\ttrain_loss = 0.0\n",
            "\ttrain_reg_loss = 0.0\n",
            "\ttrain_cls_loss = 0.0\n",
            "\ttrain_iou = 0.9702\n",
            "\ttrain_accuracy = 1.0\n",
            "\tval_loss = 0.0\n",
            "\tval_reg_loss = 0.0\n",
            "\tval_cls_loss = 0.0\n",
            "\tval_iou = 0.9754\n",
            "\tval_accuracy = 1.0\n",
            "\n"
          ]
        }
      ],
      "execution_count": 31
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save"
      ],
      "metadata": {
        "id": "OkwYxkos8ZpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, \"pretrained_model.pth\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T00:07:24.567386Z",
          "iopub.execute_input": "2025-02-11T00:07:24.567684Z",
          "iopub.status.idle": "2025-02-11T00:07:24.975753Z",
          "shell.execute_reply.started": "2025-02-11T00:07:24.56766Z",
          "shell.execute_reply": "2025-02-11T00:07:24.975105Z"
        },
        "id": "QUeJkzzo8ZpV"
      },
      "outputs": [],
      "execution_count": 32
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission"
      ],
      "metadata": {
        "id": "W5YZx1TU8ZpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform inference on cpu in order to avoid memory problems\n",
        "device = 'cuda'\n",
        "model = model.to(device)\n",
        "\n",
        "test_root_dir = osp.join(config['DATA_DIR'], \"images/images\")\n",
        "test_df = pd.read_csv(osp.join(config['DATA_DIR'], \"test.csv\"))\n",
        "\n",
        "test_ds = militarDataset(test_df, root_dir=test_root_dir, labeled=False, transform=eval_transforms,output_size=(w,h))#\n",
        "test_data = DataLoader(test_ds, batch_size=1, num_workers=cpu_count(), shuffle=False)\n",
        "\n",
        "class_preds = []\n",
        "bbox_preds = []\n",
        "\n",
        "for batch in test_data:\n",
        "    batch_preds = model(batch['image'].float().to(device))\n",
        "\n",
        "    class_pred = batch_preds['class_id'].argmax(-1).detach().cpu().numpy()\n",
        "    bbox_pred = batch_preds['bbox'].detach().cpu().numpy()\n",
        "\n",
        "    class_preds.append(class_pred.squeeze())\n",
        "    bbox_preds.append(bbox_pred.squeeze())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T00:07:29.179865Z",
          "iopub.execute_input": "2025-02-11T00:07:29.180221Z",
          "iopub.status.idle": "2025-02-11T00:07:29.853822Z",
          "shell.execute_reply.started": "2025-02-11T00:07:29.180194Z",
          "shell.execute_reply": "2025-02-11T00:07:29.852453Z"
        },
        "id": "e-YMgf1P8ZpV"
      },
      "outputs": [],
      "execution_count": 33
    },
    {
      "cell_type": "code",
      "source": [
        "class_preds = np.array(class_preds)\n",
        "bbox_preds = np.array(bbox_preds)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T00:07:37.006701Z",
          "iopub.execute_input": "2025-02-11T00:07:37.007064Z",
          "iopub.status.idle": "2025-02-11T00:07:37.012021Z",
          "shell.execute_reply.started": "2025-02-11T00:07:37.007032Z",
          "shell.execute_reply": "2025-02-11T00:07:37.011307Z"
        },
        "id": "Ia1wt2-z8ZpV"
      },
      "outputs": [],
      "execution_count": 34
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame(\n",
        "    index=test_df.filename,\n",
        "    data={\n",
        "        'class_id': class_preds,\n",
        "        }\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T00:07:39.834308Z",
          "iopub.execute_input": "2025-02-11T00:07:39.834611Z",
          "iopub.status.idle": "2025-02-11T00:07:39.839673Z",
          "shell.execute_reply.started": "2025-02-11T00:07:39.834587Z",
          "shell.execute_reply": "2025-02-11T00:07:39.838725Z"
        },
        "id": "39VZ9aVl8ZpW"
      },
      "outputs": [],
      "execution_count": 35
    },
    {
      "cell_type": "code",
      "source": [
        "submission[\"xmin\"] = bbox_preds[:, 0]*config['w_real']\n",
        "submission[\"ymin\"] = bbox_preds[:, 1]*config['h_real']\n",
        "submission[\"xmax\"] = bbox_preds[:, 2]*config['w_real']\n",
        "submission[\"ymax\"] = bbox_preds[:, 3]*config['h_real']\n",
        "submission['class']=submission['class_id'].replace(config['id2obj'])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T00:07:43.461137Z",
          "iopub.execute_input": "2025-02-11T00:07:43.461459Z",
          "iopub.status.idle": "2025-02-11T00:07:43.47164Z",
          "shell.execute_reply.started": "2025-02-11T00:07:43.461432Z",
          "shell.execute_reply": "2025-02-11T00:07:43.470496Z"
        },
        "id": "b1UcXZsi8ZpW"
      },
      "outputs": [],
      "execution_count": 36
    },
    {
      "cell_type": "code",
      "source": [
        "submission['class'].value_counts()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T00:07:46.792724Z",
          "iopub.execute_input": "2025-02-11T00:07:46.793089Z",
          "iopub.status.idle": "2025-02-11T00:07:46.804258Z",
          "shell.execute_reply.started": "2025-02-11T00:07:46.793057Z",
          "shell.execute_reply": "2025-02-11T00:07:46.803382Z"
        },
        "id": "XOLijmDp8ZpW",
        "outputId": "6900349c-d311-46eb-84aa-5d124cb77ff6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "class\n",
              "chinook    15\n",
              "f16        14\n",
              "cougar     12\n",
              "ah64       11\n",
              "seahawk     6\n",
              "f15         5\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>class</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>chinook</th>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f16</th>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cougar</th>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ah64</th>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>seahawk</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f15</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "execution_count": 37
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv('submission.csv')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-11T00:07:52.352919Z",
          "iopub.execute_input": "2025-02-11T00:07:52.353229Z",
          "iopub.status.idle": "2025-02-11T00:07:52.36351Z",
          "shell.execute_reply.started": "2025-02-11T00:07:52.353204Z",
          "shell.execute_reply": "2025-02-11T00:07:52.362514Z"
        },
        "id": "kO2olMM98ZpW"
      },
      "outputs": [],
      "execution_count": 38
    }
  ]
}